{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d61ecbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.12/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/anaconda3/lib/python3.12/site-packages (from opencv-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c696d2e-b12e-4700-8cbc-4ca835abfa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "781bc0cf-4147-4911-b480-4bc0e7099ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class BRAIT_CNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(BRAIT_CNN, self).__init__()\n",
    "    self.brait1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        nn.Dropout(p=0.01))\n",
    "    self.brait2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        nn.Dropout(p=0.01))\n",
    "    self.brait3 = nn.Sequential(\n",
    "        nn.Linear(32*7*7, 100),\n",
    "        nn.Linear(100, 26))\n",
    "\n",
    "  def forward(self, x):\n",
    "    y = F.relu(self.brait1(x))\n",
    "    y = F.relu(self.brait2(y))\n",
    "\n",
    "    #flatten\n",
    "    y = y.view(-1, 32*7*7)\n",
    "    y = F.relu(self.brait3(y))\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3e03db6-a1d9-46fd-aba7-07bc4ca6b782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "31.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f', 'a', 'm', 'i', 'l', 'y']\n",
      "4\n",
      "31.5\n",
      "['h', 'r', 'm', 'e']\n",
      "7\n",
      "76.28571428571429\n",
      "['p', 'r', 'a', 'i', 'r', 'i', 'e']\n",
      "15\n",
      "29.933333333333334\n",
      "['t', 'h', 'r', 'd', 'j', 'r', 'w', 'd', 'i', 'a', 'd', 'f', 'c', 'a', 'l']\n",
      "4\n",
      "24.75\n",
      "['f', 'o', 'o', 'd']\n",
      "5\n",
      "30.8\n",
      "['w', 'o', 'u', 'l', 'd']\n",
      "15\n",
      "31.6\n",
      "['w', 'i', 't', 'h', 'k', 'h', 'i', 's', 'j', 'f', 'a', 'm', 'i', 'l', 'y']\n",
      "10\n",
      "31.2\n",
      "['t', 'h', 'e', 'k', 'l', 'i', 't', 't', 'l', 'd']\n",
      "12\n",
      "29.5\n",
      "['l', 'i', 't', 'w', 'w', 'i', 'a', 'd', 'a', 'c', 'r', 'l']\n",
      "12\n",
      "37.75\n",
      "['t', 'a', 'd', 'd', 'a', 'e', 'a', 'h', 'o', 'a', 'e', 'a']\n",
      "8\n",
      "25.125\n",
      "['a', 'c', 'c', 'u', 'r', 'a', 'c', 'y']\n",
      "5\n",
      "77.4\n",
      "['p', 'r', 'e', 's', 's']\n",
      "4\n",
      "74.5\n",
      "['s', 'a', 'y', 's']\n",
      "4\n",
      "30.75\n",
      "['t', 'o', 'o', 'k']\n"
     ]
    }
   ],
   "source": [
    "def BRAIT_PREDICTION(img_path):\n",
    "    #load model BRAIT_CNN\n",
    "    model = BRAIT_CNN()\n",
    "    model.load_state_dict(torch.load('/Users/apple/Documents/PROJECTS/Braille_Translator/BRAIT-Machine-Learning-main/BRAIT-MODEL/BRAILLE_PYTORCH.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "    image = Image.open(img_path)\n",
    "    image = image.convert('RGB')\n",
    "\n",
    "\n",
    "    width, height = image.size \n",
    "    jumlah_segment = round(width/height/0.765) \n",
    "    print(jumlah_segment)\n",
    "    segment = width/jumlah_segment\n",
    "    print(segment)\n",
    "    \n",
    "    tamp=[]\n",
    "    for i in range (0,jumlah_segment):\n",
    "        cropped = image.crop((i*segment,0,(i+1)*segment,height))\n",
    "        cropped = np.array(cropped)\n",
    "        cropped = cv2.resize(cropped, (28, 28))\n",
    "        cropped = cropped.astype(np.float32) / 255.0\n",
    "        cropped = torch.from_numpy(cropped[None, :, :, :])\n",
    "        cropped = cropped.permute(0, 3, 1, 2)\n",
    "        predicted_tensor = model(cropped)\n",
    "        _, predicted_letter = torch.max(predicted_tensor, 1)\n",
    "                \n",
    "        if int(predicted_letter) > 25:\n",
    "            #tamp=tamp+str(chr(32))\n",
    "            tamp.append(' ')\n",
    "        else:\n",
    "            #letters = letters + letters.append(chr(97 + predicted_letter))\n",
    "            tamp.append(chr(97 + predicted_letter))\n",
    "    return tamp\n",
    "\n",
    "print(BRAIT_PREDICTION(r\"/Users/apple/Documents/PROJECTS/Braille_Translator/BRAIT-Machine-Learning-main/BRAIT-SAMPLE/family.jpg\"))\n",
    "print(BRAIT_PREDICTION(r\"/Users/apple/Documents/PROJECTS/Braille_Translator/BRAIT-Machine-Learning-main/BRAIT-SAMPLE/home.jpg\"))\n",
    "print(BRAIT_PREDICTION(r\"/Users/apple/Documents/PROJECTS/Braille_Translator/BRAIT-Machine-Learning-main/BRAIT-SAMPLE/Prairie.jpg\"))\n",
    "print(BRAIT_PREDICTION(r\"/Users/apple/Documents/PROJECTS/Braille_Translator/BRAIT-Machine-Learning-main/BRAIT-SAMPLE/threw_the_ball.png\"))\n",
    "print(BRAIT_PREDICTION(r\"/Users/apple/Documents/PROJECTS/Braille_Translator/BRAIT-Machine-Learning-main/BRAIT-SAMPLE/FOOD.png\"))\n",
    "print(BRAIT_PREDICTION(r\"/Users/apple/Documents/PROJECTS/Braille_Translator/BRAIT-Machine-Learning-main/BRAIT-SAMPLE/would.png\"))\n",
    "print(BRAIT_PREDICTION(r\"/Users/apple/Documents/PROJECTS/Braille_Translator/BRAIT-Machine-Learning-main/BRAIT-SAMPLE/with_his_family.png\"))\n",
    "print(BRAIT_PREDICTION(r\"/Users/apple/Documents/PROJECTS/Braille_Translator/BRAIT-Machine-Learning-main/BRAIT-SAMPLE/the_little.png\"))\n",
    "print(BRAIT_PREDICTION(r\"/Users/apple/Documents/PROJECTS/Braille_Translator/BRAIT-Machine-Learning-main/BRAIT-SAMPLE/little_girl.png\"))\n",
    "print(BRAIT_PREDICTION(r\"/Users/apple/Documents/PROJECTS/Braille_Translator/BRAIT-Machine-Learning-main/BRAIT-SAMPLE/i_am_rayyan_mirza.png\"))\n",
    "print(BRAIT_PREDICTION(r\"/Users/apple/Documents/PROJECTS/Braille_Translator/BRAIT-Machine-Learning-main/BRAIT-SAMPLE/accuracy.png\"))\n",
    "print(BRAIT_PREDICTION(r\"/Users/apple/Documents/PROJECTS/Braille_Translator/BRAIT-Machine-Learning-main/BRAIT-SAMPLE/PRESS.jpg\"))\n",
    "print(BRAIT_PREDICTION(r\"/Users/apple/Documents/PROJECTS/Braille_Translator/BRAIT-Machine-Learning-main/BRAIT-SAMPLE/says.jpg\"))\n",
    "print(BRAIT_PREDICTION(r\"/Users/apple/Documents/PROJECTS/Braille_Translator/BRAIT-Machine-Learning-main/BRAIT-SAMPLE/took.jpg\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e723e6-82ca-4cf0-941d-7ccb67b03bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
